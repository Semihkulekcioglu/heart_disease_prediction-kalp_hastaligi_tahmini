"""
Model YardÄ±mcÄ± FonksiyonlarÄ±
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
from sklearn.model_selection import cross_val_score, GridSearchCV
import joblib
import os


def train_model(model, X_train, y_train, model_name="Model"):
    """
    Modeli eÄŸitir
    
    Parameters:
    -----------
    model : estimator
        EÄŸitilecek model
    X_train : array-like
        EÄŸitim Ã¶zellikleri
    y_train : array-like
        EÄŸitim hedef deÄŸiÅŸkeni
    model_name : str
        Model adÄ±
    
    Returns:
    --------
    model : estimator
        EÄŸitilmiÅŸ model
    """
    print(f"\n{'='*60}")
    print(f"{model_name} EÄžÄ°TÄ°LÄ°YOR...")
    print(f"{'='*60}")
    
    model.fit(X_train, y_train)
    
    print(f"âœ“ {model_name} baÅŸarÄ±yla eÄŸitildi!")
    return model


def evaluate_model(model, X_train, X_test, y_train, y_test, model_name="Model"):
    """
    Model performansÄ±nÄ± deÄŸerlendirir
    
    Parameters:
    -----------
    model : estimator
        DeÄŸerlendirilecek model
    X_train, X_test : array-like
        EÄŸitim ve test Ã¶zellikleri
    y_train, y_test : array-like
        EÄŸitim ve test hedef deÄŸiÅŸkenleri
    model_name : str
        Model adÄ±
    
    Returns:
    --------
    results : dict
        Performans metrikleri
    """
    # Tahminler
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # OlasÄ±lÄ±k tahminleri (varsa)
    try:
        y_train_proba = model.predict_proba(X_train)[:, 1]
        y_test_proba = model.predict_proba(X_test)[:, 1]
        has_proba = True
    except:
        has_proba = False
    
    # Metrikler
    results = {
        'train_accuracy': accuracy_score(y_train, y_train_pred),
        'test_accuracy': accuracy_score(y_test, y_test_pred),
        'train_precision': precision_score(y_train, y_train_pred),
        'test_precision': precision_score(y_test, y_test_pred),
        'train_recall': recall_score(y_train, y_train_pred),
        'test_recall': recall_score(y_test, y_test_pred),
        'train_f1': f1_score(y_train, y_train_pred),
        'test_f1': f1_score(y_test, y_test_pred),
    }
    
    if has_proba:
        results['train_roc_auc'] = roc_auc_score(y_train, y_train_proba)
        results['test_roc_auc'] = roc_auc_score(y_test, y_test_proba)
    
    # SonuÃ§larÄ± yazdÄ±r
    print(f"\n{'='*60}")
    print(f"{model_name} PERFORMANS METRÄ°KLERÄ°")
    print(f"{'='*60}")
    print(f"\n{'Metrik':<20} {'EÄŸitim':<15} {'Test':<15}")
    print("-" * 60)
    print(f"{'Accuracy':<20} {results['train_accuracy']:.4f}{'':<10} {results['test_accuracy']:.4f}")
    print(f"{'Precision':<20} {results['train_precision']:.4f}{'':<10} {results['test_precision']:.4f}")
    print(f"{'Recall':<20} {results['train_recall']:.4f}{'':<10} {results['test_recall']:.4f}")
    print(f"{'F1-Score':<20} {results['train_f1']:.4f}{'':<10} {results['test_f1']:.4f}")
    if has_proba:
        print(f"{'ROC-AUC':<20} {results['train_roc_auc']:.4f}{'':<10} {results['test_roc_auc']:.4f}")
    print("=" * 60)
    
    return results


def plot_confusion_matrix(y_true, y_pred, model_name="Model", figsize=(8, 6)):
    """
    Confusion matrix'i gÃ¶rselleÅŸtirir
    
    Parameters:
    -----------
    y_true : array-like
        GerÃ§ek deÄŸerler
    y_pred : array-like
        Tahmin edilen deÄŸerler
    model_name : str
        Model adÄ±
    figsize : tuple
        Grafik boyutu
    """
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=figsize)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['SaÄŸlÄ±klÄ±', 'Hasta'], 
                yticklabels=['SaÄŸlÄ±klÄ±', 'Hasta'])
    plt.title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')
    plt.ylabel('GerÃ§ek DeÄŸer', fontsize=12)
    plt.xlabel('Tahmin', fontsize=12)
    plt.tight_layout()
    plt.show()
    
    # DetaylÄ± bilgi
    tn, fp, fn, tp = cm.ravel()
    print(f"\nConfusion Matrix DetaylarÄ±:")
    print(f"  True Negatives (TN):  {tn}")
    print(f"  False Positives (FP): {fp}")
    print(f"  False Negatives (FN): {fn}")
    print(f"  True Positives (TP):  {tp}")


def plot_roc_curve(y_true, y_proba, model_name="Model", figsize=(8, 6)):
    """
    ROC eÄŸrisini Ã§izer
    
    Parameters:
    -----------
    y_true : array-like
        GerÃ§ek deÄŸerler
    y_proba : array-like
        Pozitif sÄ±nÄ±f olasÄ±lÄ±klarÄ±
    model_name : str
        Model adÄ±
    figsize : tuple
        Grafik boyutu
    """
    fpr, tpr, thresholds = roc_curve(y_true, y_proba)
    roc_auc = roc_auc_score(y_true, y_proba)
    
    plt.figure(figsize=figsize)
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title(f'{model_name} - ROC Curve', fontsize=14, fontweight='bold')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()


def cross_validate_model(model, X, y, cv=5, scoring='accuracy', model_name="Model"):
    """
    Cross-validation ile model performansÄ±nÄ± deÄŸerlendirir
    
    Parameters:
    -----------
    model : estimator
        DeÄŸerlendirilecek model
    X : array-like
        Ã–zellikler
    y : array-like
        Hedef deÄŸiÅŸken
    cv : int
        Fold sayÄ±sÄ±
    scoring : str
        DeÄŸerlendirme metriÄŸi
    model_name : str
        Model adÄ±
    
    Returns:
    --------
    scores : array
        CV skorlarÄ±
    """
    print(f"\n{model_name} - {cv}-Fold Cross Validation yapÄ±lÄ±yor...")
    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
    
    print(f"\nCross Validation SonuÃ§larÄ±:")
    print(f"  Skorlar: {scores}")
    print(f"  Ortalama: {scores.mean():.4f}")
    print(f"  Standart Sapma: {scores.std():.4f}")
    
    return scores


def tune_hyperparameters(model, param_grid, X_train, y_train, cv=5, scoring='accuracy', model_name="Model"):
    """
    GridSearchCV ile hiperparametre optimizasyonu yapar
    
    Parameters:
    -----------
    model : estimator
        Optimize edilecek model
    param_grid : dict
        Parametre grid'i
    X_train : array-like
        EÄŸitim Ã¶zellikleri
    y_train : array-like
        EÄŸitim hedef deÄŸiÅŸkeni
    cv : int
        Fold sayÄ±sÄ±
    scoring : str
        DeÄŸerlendirme metriÄŸi
    model_name : str
        Model adÄ±
    
    Returns:
    --------
    best_model : estimator
        En iyi model
    best_params : dict
        En iyi parametreler
    """
    print(f"\n{'='*60}")
    print(f"{model_name} - HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU")
    print(f"{'='*60}")
    print(f"Toplam {len(param_grid)} parametre test edilecek...")
    
    grid_search = GridSearchCV(
        model, param_grid, cv=cv, scoring=scoring, 
        n_jobs=-1, verbose=1, return_train_score=True
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"\nâœ“ Optimizasyon tamamlandÄ±!")
    print(f"\nEn Ä°yi Parametreler:")
    for param, value in grid_search.best_params_.items():
        print(f"  {param}: {value}")
    print(f"\nEn Ä°yi {scoring.capitalize()} Skoru: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_, grid_search.best_params_


def save_model(model, filename, folder='../models'):
    """
    Modeli kaydeder
    
    Parameters:
    -----------
    model : estimator
        Kaydedilecek model
    filename : str
        Dosya adÄ±
    folder : str
        KlasÃ¶r yolu
    """
    if not os.path.exists(folder):
        os.makedirs(folder)
    
    filepath = os.path.join(folder, filename)
    joblib.dump(model, filepath)
    print(f"âœ“ Model kaydedildi: {filepath}")


def load_model(filename, folder='../models'):
    """
    KaydedilmiÅŸ modeli yÃ¼kler
    
    Parameters:
    -----------
    filename : str
        Dosya adÄ±
    folder : str
        KlasÃ¶r yolu
    
    Returns:
    --------
    model : estimator
        YÃ¼klenen model
    """
    filepath = os.path.join(folder, filename)
    model = joblib.load(filepath)
    print(f"âœ“ Model yÃ¼klendi: {filepath}")
    return model


def plot_feature_importance(model, feature_names, top_n=None, figsize=(10, 6)):
    """
    Feature importance'Ä± gÃ¶rselleÅŸtirir
    
    Parameters:
    -----------
    model : estimator
        Feature importance'a sahip model
    feature_names : list
        Ã–zellik isimleri
    top_n : int, optional
        GÃ¶sterilecek en Ã¶nemli n Ã¶zellik
    figsize : tuple
        Grafik boyutu
    """
    try:
        importances = model.feature_importances_
    except AttributeError:
        print("âš  Bu model feature importance desteklemiyor.")
        return
    
    # DataFrame oluÅŸtur
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    if top_n:
        feature_importance_df = feature_importance_df.head(top_n)
        title = f'Top {top_n} Feature Importance'
    else:
        title = 'Feature Importance'
    
    # GÃ¶rselleÅŸtirme
    plt.figure(figsize=figsize)
    sns.barplot(data=feature_importance_df, x='importance', y='feature', palette='viridis')
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel('Importance', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.tight_layout()
    plt.show()
    
    return feature_importance_df


def compare_models(results_dict, metric='test_accuracy', figsize=(12, 6)):
    """
    FarklÄ± modelleri karÅŸÄ±laÅŸtÄ±rÄ±r
    
    Parameters:
    -----------
    results_dict : dict
        Model adÄ±: sonuÃ§lar sÃ¶zlÃ¼ÄŸÃ¼
    metric : str
        KarÅŸÄ±laÅŸtÄ±rÄ±lacak metrik
    figsize : tuple
        Grafik boyutu
    """
    models = list(results_dict.keys())
    scores = [results_dict[model][metric] for model in models]
    
    plt.figure(figsize=figsize)
    bars = plt.bar(models, scores, color='skyblue', edgecolor='navy', alpha=0.7)
    
    # En iyi modeli vurgula
    best_idx = np.argmax(scores)
    bars[best_idx].set_color('orange')
    
    plt.title(f'Model KarÅŸÄ±laÅŸtÄ±rmasÄ± - {metric.replace("_", " ").title()}', 
              fontsize=14, fontweight='bold')
    plt.ylabel('Score', fontsize=12)
    plt.xlabel('Model', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.ylim([min(scores) - 0.05, max(scores) + 0.05])
    
    # DeÄŸerleri yazdÄ±r
    for i, (model, score) in enumerate(zip(models, scores)):
        plt.text(i, score + 0.01, f'{score:.4f}', ha='center', fontweight='bold')
    
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # En iyi modeli belirt
    best_model = models[best_idx]
    print(f"\nðŸ† En iyi model: {best_model} ({metric}: {scores[best_idx]:.4f})")


